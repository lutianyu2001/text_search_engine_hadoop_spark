{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF for small_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import os, re, time, math\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = r'/root/workshop32/dataset/'\n",
    "# ensure you copy all the datafiles into this position in all the spark clusters !\n",
    "# copy by scp: scp -r /root/workshop32 root@workshop32slave2:/root/\n",
    "\n",
    "path = r'hdfs://workshop32master:9000/workshop32/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/15 21:48:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://workshop32master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://workshop32master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyTf-Idf</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://workshop32master:7077 appName=MyTf-Idf>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '8g')\n",
    "SparkContext.setSystemProperty('spark.driver.memory', '8g')\n",
    "spark = SparkSession.builder.master('spark://workshop32master:7077').appName('MyTf-Idf').getOrCreate()\n",
    "# use spark cluster\n",
    "    \n",
    "sc = spark.sparkContext\n",
    "#sc = SparkContext.getOrCreate(SparkConf())\n",
    "# sc._conf.set('spark.executor.memory', '8g')\n",
    "# sc._conf.set('spark.driver.memory', '8g')\n",
    "# sc._conf.set('spark.driver.maxResultsSize', '0')\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time:  61.96755385398865\n"
     ]
    }
   ],
   "source": [
    "# sparkIndex\n",
    "data = sc.wholeTextFiles(path+'small_set')\n",
    "\n",
    "numFiles = data.count()\n",
    "\n",
    "timeStart = time.time()\n",
    "wordcount = data.flatMap(lambda x: [((os.path.basename(x[0]), i), 1) \\\n",
    "        for i in re.split('\\\\W', x[1])]) \\\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "tf = wordcount.map(lambda x: (x[0][1].lower(),(x[0][0],x[1])))\n",
    "tf_list = tf.collect()\n",
    "timeEnd = time.time()\n",
    "\n",
    "print(\"Execution Time: \",(timeEnd - timeStart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', ('1000.txt', 93294)),\n",
       " ('the', ('1000.txt', 21)),\n",
       " ('project', ('1000.txt', 29)),\n",
       " ('etext', ('1000.txt', 5)),\n",
       " ('dante', ('1000.txt', 44)),\n",
       " ('no', ('1000.txt', 21)),\n",
       " ('please', ('1000.txt', 3)),\n",
       " ('notes', ('1000.txt', 1)),\n",
       " ('versions', ('1000.txt', 3)),\n",
       " ('beneath', ('1000.txt', 1))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time:  26.089613676071167\n"
     ]
    }
   ],
   "source": [
    "timeStart = time.time()\n",
    "tf2 = tf.map(lambda x: (x[0],(x[1][0], 1+math.log10((x[1][1]) ))))\n",
    "\n",
    "idf = wordcount.map(lambda x: (x[0][1], (x[0][0], x[1], 1)))\\\n",
    "        .map(lambda x: (x[0], x[1][2]))\\\n",
    "        .reduceByKey(lambda x, y: x + y)\\\n",
    "        .map(lambda x: (x[0], math.log10(numFiles / (x[1]))))\n",
    "\n",
    "tfidf = tf2.join(idf)\n",
    "tfidf = tfidf.map(lambda x: (x[1][0][0], (x[0], x[1][0][1] * x[1][1]))).sortByKey()\n",
    "index = tfidf.map(lambda x:(x[1][0], (x[0], x[1][1]))).filter(lambda x: x[0]!='')\n",
    "index = index.sortByKey()\n",
    "lines = index.map(lambda x: (x[0], x[1][0], x[1][1]))\n",
    "lines_list = lines.collect()\n",
    "\n",
    "ranking_dict = {}\n",
    "for i in lines_list:\n",
    "    ranking_dict[i[0]] = ranking_dict.get(i[0],[]) + [(i[1],i[2])]\n",
    "    \n",
    "for i in ranking_dict.items():\n",
    "    i[1].sort(key = lambda x:x[1], reverse = True)\n",
    "    ranking_dict[i[0]] = i[1]\n",
    "\n",
    "with open(\"tfidf_index_small.txt\", \"w\") as f:\n",
    "    for i in ranking_dict.items():\n",
    "        # to write in a specific way <word>:[]\n",
    "        f.write(i[0]+\":[\")\n",
    "        for j in range(len(i[1])):\n",
    "            f.write(str(i[1][j][0]))\n",
    "            if(j == len(i[1])-1):\n",
    "                f.write('('+str(i[1][j][1])+\")]\\n\")\n",
    "            else:\n",
    "                f.write('('+str(i[1][j][1])+\"),\")\n",
    "        \n",
    "timeEnd = time.time()\n",
    "\n",
    "print(\"Execution Time: \",(timeEnd - timeStart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('monomaniac', [('1024.txt', 2.001445240874181), ('1075.txt', 2.001445240874181), ('1264.txt', 2.001445240874181)])\n"
     ]
    }
   ],
   "source": [
    "# FOR DEBUG\n",
    "print([i for i in ranking_dict.items()][66654])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code for retreiving the tfidf index from tfidf_index.txt\n",
    "\n",
    "# tfidf_open = []\n",
    "\n",
    "# with open(\"tfidf_index.txt\", \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         line = line.strip('\\n')\n",
    "#         tfidf_open.append(eval(line))\n",
    "\n",
    "# tfidf_open[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR DEBUG: getting a slice of tfidf_index\n",
    "# with open(\"tfidf_index_slice.txt\", \"w\") as f:\n",
    "#     for j in range(66666, 67666):\n",
    "#         i = tfidf_open[j]\n",
    "#         f.write(str(i)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
